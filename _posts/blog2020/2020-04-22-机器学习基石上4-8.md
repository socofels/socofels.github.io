---
layout: article
title: 机器学习基石上4-8
tags: 机器学习基石
aside:
  toc: true
article_header:
  type: cover
  image:
    src: assets/images/background/pic/sky.jpg
mathjax: true
mathjax_autoNumber: true
---
# 学习的核心问题是什么
* $E_{out}$与$E_{in}$是否接近
* $E_{in}(g)$是否足够小
## M的大小,有什么影响
$P[|E_{in}-E_{out}|>\epsilon]\leq 2Mexp(-2\epsilon^2N)$
<!--more-->
## 那么如何获得更精确的M?
1. **首先我们回顾一下,M是如何得到的**
    * $$P_D[BAD D]=[badD for h_1 or badD for h_2 or badD for h_3 ........badD for h_m]$$
    - 把or转为+,那么得到不等式
    * $$P_D[BAD D]\leq P_D[badD for h_1]+P_D[badD for h_2]+.......P_D[badD for h_m]$$
    * $$P[|E_{in}-E_{out}|>\epsilon]\leq 2Mexp(-2\epsilon^2N)$$
* 这里有个点,是什么呢?我们取得上限badd for h_1.........,然而实际上,这些错误的数据额几并不是完全分散的,他们有很多是重叠的,但我们把这些重叠部分也算了一次.所以得到的M,会使得变坏几率变无限大
* 好那么接下来,我们可以这样认为,h可以有很多,但是,他们有的非常接近的,我们可以把它化为一种,那么到底有多少种呢?如何划分呢?接下来我们讨论这个问题

#### N个数据,每个数据都可能是0或1,那么到底有多少种猜想,可以把这些数据用一条线一分为二呢?
* 我们用$m_H(N)$来表示有多少种不同的猜想(线).
* 首先,我们知道N个数据就有$2^N$种组合,那么N个数据,我们用线分开他们的话,$m_H(N)\leq 2^N$
* 那么这个成长函数$m_H(N)\leq 2^N$ 它大小跟什么有关,我们可以求得他的大小吗?
    - 顶多一种组合对应一个猜想
* 我们提出一个概念break point,什么是break point
    - $m_H(N)\leq 2^N$,当N=break point时,$m_H(N)< 2^{break point}$严格的小于
        - 原来是一种猜想对应一种组合，现在有些组合是做不到线性可以分，所以猜想数肯定严格小于组合数了。
    - 就是说如果一维平面下,有三个点,用直线把他们线性可分.$2^3$有8种结果,但是有两种情况是不能够线性可分的XOX,OXO.我们把这两个点称之为break point.
    - 类似的在二维平面中,break point 就是4,第一个不能全部二分的点。
* 当出现break point 的时候,也就是说,在$2^N$种猜想中,有一部分是必然无法实现的,而且我们知道,既然它无法实现,那么在这个无法实现的猜想中加无论多少的点,依然是不能实现的,所以$N>break point时,肯定也有m_H(N)\leq 2^N$
* 那么我们再看在不同情况下,不同种猜想$m_H(N)$有多少？
    - 在一维中,用线来二分  $m_H(N)\leq N+1$
    - 在一维,我们用一段来划分区域正负 $m_H(N)\leq \frac{N^2}{2} + \frac{N}{2} + 1$
    - 在二维中,如果所有的点在一个圆上,我们用一个区域来表示正负范围,那么我们有 $m_H(N)\leq 2^N$
    - $B(N,k)$表示的是N个数据，它的break point是K的情况下，一共有$B(N,k)$种猜想可以将数据一分为二。这个可以通过列除表格，数学归纳法得到结论。
    - 如果是在二维平面中,我们用线来划分正负,经过一系列计算,我们知道有$m_H(N)\leq B(N-1,k)+B(N-1,k-1)$,$m_H(N)\leq B(N-1,k)+B(N-1,k-1)$太麻烦,当N足够大的时候哦,我们发现$m_H(N)\leq N^{k-1}$,于是我们用它来代替$m_H(N)$
* 有了这些推论后,我们带入式子发现,这样一来的话,N增大,两个因式一个是增大,一个减小.
    - $$P[|E_{in}-E_{out}|>\epsilon]\leq 2 N^{k-1}exp(-2\epsilon^2N)$$
- 我们由这些可以统计得到满足三个条件,我们似乎就能让机器学习到好的算法.
>    - $m(m_H(N))$ 存在break point 时
>    - N足够大,也是说D足够大时(probaly)
>    - 在配合上好的算法(probaly)
>    - 再碰上一点好运气

## 准确的E_out
有一个问题,我们使用霍夫丁公式时,我们无法知道EOUT,因为它有无限个样本,那么我们有没有办法用有限的样本得到一个数来取代EOUT呢?
* 办法是有的
* 证明办法暂时无法理解,通过的是不放回的霍夫丁模型,求得V-C上界制（Vapnik-Chervonenkis bound）
* 最终的公式,我们修改修改为
    - $$p[|E_{in}-E_{out}|< \epsilon]\leq 4m_H(2N)exp(-\frac{1}{8}\epsilon^2N)$$


### 什么是vc,有什么用?
- $d_{vc} = Minnum(break point) -1$
- 意思就是在这个点以内，全部能shatter
* 这个vc有什么用?
    - 我们想用这个vc,看看能否与我们的$E_{in},E_{out}相关联,得到他们之间的关系$
* 我们有了$d{vc}$之后,有些什么变化?
    - 在一维中,用线来二分  $m_H(N)\leq N+1$
        - $d{vc}=1$
    - 在一维,我们用一段来划分区域正负 $m_H(N)\leq \frac{N^2}{2} + \frac{N}{2} + 1$
        - $d{vc}=2$
    - 在二维中,如果所有的点在一个圆上,我们用一个区域来表示正负范围,那么我们有 $m_H(N)\leq 2^N$
        - $d{vc}=\infty$
    - 如果是在二维平面中,$m_H(N)\leq N^{3}$   对于N>2时
        - $d{vc}=3$
* 结合上面这些情况与公式
- $$P[|E_{in}-E_{out}|>\epsilon]\leq 2 m_H(N)exp(-2\epsilon^2N)$$
* $d_{vc}$是有限的，那么可以得到$2 m_H(N)exp(-2\epsilon^2N)$是一个比较小的值，则$E_{in}$与$E_{out}$是接近的。并且这个结论与其他外在条件没有关系，跟算法没关系，跟数据分布没关系，跟目标函数是什么样的也没关系。所以可以说明,在最坏的情况下，也有能做到$E_{in}$与$E_{out}$是接近的。也就是说我们训练出来的模型，pred与y是一样的。

### 现在我们知道了一维二维中的$d{vc}$,那再高维的情况下,又是如何?
- 先通过1,2,维,我们看起来$d{vc}=d+1$好像是这个规律,那么我们如何证明它?
    - 要证明他们的思路就是用夹逼法,来证明
        - $d{vc}\geq d + 1$
        - $d{vc}\leq d + 1$
- 第一步证明$d{vc}\geq d + 1$
    * 画出矩阵,我们知道有($y = Wx$)  ==>  ($W = x^{-1}y$)
    * W是$d_{vc}$维的,所以W一定是大于等于$d_{vc}$的
- 第二步证明$d{vc}< d + 2$
    * 当$d{vc}= d + 2$时,X的向量中一定有一条能够被其他向量线性表出,那么意味着$W_{d+2}$被其他d+1个决定了,我们不能够任意的选择它的正负,说明了$d{vc}< d + 2$
* 所得得到了$d{vc}=d+1$
- 根据这些关系,我们得到图
    * ![image](https://socofels.github.io/assets/images/blogimg/ml.jpg)

* 图中的核心思想就是告诉我们,不要一昧的追求低E_in,因为它不见得是非常好的选择,我们要看能付出多少代价
* 另外我们通过这个公式,可以得到一个N,不过实际上我们选择的N比计算得到的这个数值要低很多,选择范围宽松.
# **通过以上结论,我们论证了对于PLA问题机器学习的可行性.**

# 杂讯
我们以上的论证都是在没有杂讯的情况下,那么实际中,杂讯是不可避免的,那么在有杂讯时,机器能不能够学习到东西呢?
- 同样的我们把抽弹珠表示$E_{in}$与$E_{out}$,把杂讯当做是变色弹珠,我们无法知道它是正还是负,但是在拿到他的那一刻,是什么,我们就认为他是什么,然后再使用VC bound 去计算变色弹珠.
* 那么最后我们就可以知道,变色弹珠的比例,把变色弹珠看做是错误的一部分,我们再非变色弹珠上做得很好,$E_{IN}$很小的话.我们就可以把这些点分成重要和不重要的点.

### 衡量这个错误ERROR
- 有不同的方法衡量,其实就是loss函数，或者叫cost函数，在基石课里叫它error，简称err
    - $$y==y?$$
    - $$|y-y|$$
    - $$(y-y)^2$$
* 给一些简单的错误衡量问题
* 然后我们给错误进行加权,能够优化一些资料不平衡导致算法失效的问题
