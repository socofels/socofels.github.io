---
layout: article
title: 机器学习基石9-12
tags: 机器学习基石
aside:
  toc: true
article_header:
  type: cover
  image:
    src: assets/images/background/pic/sky.jpg
mathjax: true
mathjax_autoNumber: true
---
## 第九章

### 这一节研究的问题是什么?机器学习在面对线性回归的问题如何解决?
### 思路
#### 总体思路:得到函数h(x)和误差分析函数$E_{in}$,得到$E_{in}$最小的情况下的W.
1. 构建函数,我们需要找到一个权重W,使得误差分析函数$E_{in}=0$
* 函数的构建很简单,以及该函数的误差分析函数$E_{in}$
    - $h(x)=X^TW$
    - $E_{in}=(h(x)-y)^2$
* 有了这两个式子,我们知道,当$E_{in}=0$时,我们就得到了想要的解,W.我们知道,误差分析函数$E_{in}$就是每个点,离这条线的距离,那么要使得$E_{in}=0$,那就要求所有点都在一条线上,而实际上,这些点都是分散的,一般不会再一条线上,那么我们需要办法,找到误差分析函数$E_{in}$中最小的那个.
* 把上面一式带入二式子得到
    - $E_{in}=(X^TW-y)^2$
* 有了这个式子后,我们首先要明白他的物理意义,实际上在2维图上就是一个抛物线,凸函数,那么我们怎样找到它的最低点呢?我们知道凸函数的导数为0时,就是最小值,在多维时,每一维度的$E_{in}(W_n)$都是0,那么说干就干
    - 我们来求$E_{in}$的导数,用$\nabla E_{in}$来表示
    - $\nabla E_{in}=\frac{2}{N}(X^TXW -X^Ty)$
    - $\nabla$ 符号代表算子,向量微分算子,在一维表示倒数,在多维表示梯度
* 找到梯度$\nabla E_{in}=0$时,的W.
    - $0=\frac{2}{N}(X^TXW -X^Ty)$
    可以得到
    - $W_{lin}=(X^TX)^{-1}X^Ty$
    注意:这里式子中的$(X^TX)^{-1}X^T$我们把它称之为pseudo-inverse(伪逆矩阵),用符号$X^+$表示.*这里特别之处在于,当$(X^TX)$为不可逆矩阵时,这个$X^+$会是一个同等效用的矩阵,从而在设计程序时,不用去考虑$(X^TX)$可不可逆的这种问题.*
* 最终就可以得到公式
    - $W_{lin} = X^+y$

<!--more-->
#### 那么线性回归问题就解决了吗?我们讨论的这些没有考虑到杂讯,当有杂讯的时候,又会是什么样子呢?
1. 有学者对错误率的平均值进行了研究,发现规律
    - $\bar{E_{in}}=noise level\cdot(1-\frac{d+1}{N})$
    - $\bar{E_{out}}=noise level\cdot (1+\frac{d+1}{N})$
    - 这两个函式子意味着,当N越来越大时,$\bar{E_{in}}$会会越来越小$\bar{E_{out}}$会越来越大,他们就会越来越靠近.

#### 拓展
* 线性回归问题会得到实数的答案,而二分类问题会得到+1与-1的答案,我们画出他他们的错误边界,可以知道,线性回归问题的错误分析函数比而分类问题的错误分析函数要大,这意味着我们可以用线性回归的错误分析函数来代替二分类的错误分析函数.

# logistic regression 逻辑线性回归
## 我们要知道预测病人得病的概率为多少,这个如何求?
## 思路
* 还是一样,得到h(x),得到错误分析函数$E_{in}$,找到$E_{in}$最小值相对于应的W

1. 我们要知道概率,那么就是要将线性回归得到的分数,转为概率,这里我们可以采用sigmoid函数,将任意实数返回区间(-1,1)的实数.sigmoid函数如下:
    - $\theta(s)=\frac{1}{1-e^{(-s)}}$
* 将得分$W^TX带入sigmoid函数就可以得到:$
    - $h(x)=\frac{1}{1-e^{(-W^TX)}}$
* 接下来我们求它的错误分析函数$E_{in}$
    1. 首先,假设f存在,那么$f(x_n)$就是样本$x_n$在y上为正时的概率
    - 我们假设有一个克隆人h,它跟f一模一样,那我们反过来推,它产生数据集的概率跟f也是一模一样,就可以得到
        - $P(x_1)f(x_1)*....*P(x_n)f(x_n)=P(x_1)h(x_1)*...*P(x_n)h(x_n)$
    - 这里有一点要提一下,因为sigmoid函数
        $\because\theta(-s) = - \theta (s)$
        $\therefore1 - h(x) = h(-x)$

    - 得到h的概率为$P(x_1)h(x_1)*...*P(x_n)h(1-h(x))$
    - 我们用$likelihood(h)$表示复制人产生数据D的概率是多少
    - $likelihood(h) =P(x_1)h(x_1)*...*P(x_n)h(h(-x_n))$
    - max $likelihood(h)\propto \prod^N_{n=1}h(y_nx_n) $ 这一步我们把常数$P(X_1)*..*P(X_n)$去掉了

    - $likelihood(h)\propto min\frac{1}{N}\sum^N_{n=1}-ln\theta(y_nW^Tx_n)$ 这一步由上式子三个动作得到,最大改为最小:乘以-1,取对数ln,$hx=(y_nW^TX_n)$,增加一个常数$\frac{1}{N}$
    - 最终得到他的错误分析函数$err(W,x,y)=ln(1+exp(-yWX))$
    - min$E_in(w) =\frac{1}/{N}\sum^{N}_{n=1}ln(1+exp(-y_nW^TX_n))$
* 由已知结论可得,min$E_in(w) =\frac{1}/{N}\sum^{N}_{n=1}ln(1+exp(-y_nW^TX_n))$是一个凸曲线,那么我们还是走之前的老路,对他求导,导数为0时就有最小值.求导后有函数
    - $\nabla E_{in}=\frac{1}{N}\sum^N_{n=1}\theta(-y_nW^TX_n)(-y_nx_n)$
    - 对这个式子进行分析,我们能否找到$\nabla E_{in}=0$的点呢?
    -$\nabla E_{in}=\frac{1}{N}\sum^N_{n=1}\theta(-y_nW^TX_n)(-y_nx_n) = 0$
        * 意味着$\theta(-y_nW^TX_n)\to 0$
        * $-y_nW^TX_n \to -\infty$
        * $y_nW^TX_n \to \infty$
* 这意味着所有点必须线性可分,这通常是做不到的,我们可以想到之前的PLA与pockert方法,知错能改,错一个改一个,试试看能否运用到这里来.
    - $W_{t+1}\gets w_t + \eta v$ 这里 $\eta$就是步长,v就是方向求梯度.
    - v = $-\nabla E_{in}$  这个物理意义就是,往梯度的反方向走.

# 线性模型的分类问题.
## 原来我们的分类问题是什么样的?
* $h(x) =sign(W^Tx)$
* 当h = y时$err= 0 $;当$h \neq y$时$err= 1 $
* 我们用新的$\hat{err}$来作为err的上届,我们使用特定的函数,称为sce
    - $\hat{err}=log_2(1+exp(-ys))$

# 逻辑回归问题
## 梯度下降
### 四个点出发,h(x),E(in),梯度,求得W
1. 逻辑回归引入sigmoid函数,先用线性回归算出分数,再用sigmoid换成$(-1,1)$的值,这里的sigmoid类似于sign,那么就有了$H(x)= sigmoid(W^TX)$
* 分析它的误差E(in)
    通过一系列转换,得到了E(in) = $\frac{1}{N}\sum^N_{n=1}ln(1+e^{-y_nW^TX_n})$
* 梯度$\nabla E_{in}=\frac{1}{N}\sum^N_{n=1}\theta(-y_nW^TX_n)(-y_nX_n)$
* $W_{t+1} = W_t - \eta V$  这里$\eta$就是步长,$V$就是方向的向量
    - $W_{t+1} = W_t - \eta \nabla E_{in}$
    - $W_{t+1} = W_t - \eta(\frac{1}{N}\sum^N_{n=1}\theta(-y_nW^TX_n)(-y_nX_n))$

### 对于梯度下降的优化,随机梯度下降
1. 随机梯度的核心就在于获得的梯度grad,$\nabla E_{in}$,我们不是对所有的点都取值,而是随机找一个点,对这个点的梯度

# 需要拓展的问题
* <span id="拓展">矩阵的梯度怎么计算</span>
