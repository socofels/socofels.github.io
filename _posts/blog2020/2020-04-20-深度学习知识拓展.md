---
layout: article
title: index
tags: Xmind
aside:
  toc: true
article_header:
  type: cover
  image:
    src: assets/images/background/pic/sky.jpg
---
softmax函数导数的推导
什么是过拟合？深度学习解决过拟合的方法有哪些
深度模型参数调整的一般方法论？
简述了解的优化器，发展综述
模型网络对称性的问题
模型参数初始化
深度模型参数调整的一般方法论？
如何减少参数
梯度消失和梯度爆炸及解决办法
如何解决梯度消失和梯度爆炸
Dropout为何能防止过拟合？
BatchNorm原理
常用的几个模型
CNN卷积-inception网络构造与历史
给定卷积核的尺寸，特征图大小计算方法？
请简要介绍下Tensorflow的计算图。
神经网络不收敛不学习的原因

什么是卷积？
什么是CNN的池化
简述下什么是生成对抗网络？
请介绍下tensorflow的计算图
deeplearning 调参经验？
LSTM为什么比RNN好？
Sigmiod、Relu、Tanh三个激活函数的缺点和不足，有没有更好的激活函数？
为什么引入非线性激活函数？
relu为何好过sigmoid和tanh？
为什么LSTM中既存在tanh和sigmoid，而不同意采用一样的。
如何解决RNN梯度消失和弥散的情况？
什么样的资料集不适合深度学习？
如何解决梯度消失和梯度爆炸？
CNN常用的几个简单模型
梯度爆炸会引发什么？
什么是RNN
什么是LSTM网络？
详细说说CNN工作原理

推导反向传播算法
Relu激活函数在零点是否可导？
Relu在零点不可导，那么在反向传播中怎么处理？
如何处理神经网络中的过拟合问题？
Relu激活函数的优缺点？
Dropout在训练的过程中会随机去掉神经元，那么在编码过程中是怎么处理的呢？
dropout的训练过程需要做rescale，这个过程是什么样子的呢？
dropout方法在预测过程中需要如何处理？
softmax的公式是什么？实际使用中会有什么问题？如何解决？
CNN中padding的作用是什么？
梯度消失和梯度爆炸的问题是如何产生的？如何解决？
语言模型中，Bert为什么在masked language model中采用了80%、10%、10%的策略？
Bert现有的问题有哪些？
非平衡数据集的处理方法有哪些？
CRF与HMM模型的区别
交叉熵损失与KL散度的区别？
为什么LSTM相比RNN能够解决long-range dependency的问题？
<!--more-->

---
1.列举常见的一些范数及其应用场景，如L0，L1，L2，L∞，Frobenius范数

2.简单介绍一下贝叶斯概率与频率派概率，以及在统计中对于真实参数的假设。

3.概率密度的万能近似器

4.简单介绍一下sigmoid，relu，softplus，tanh，RBF及其应用场景

5.Jacobian，Hessian矩阵及其在深度学习中的重要性

6.KL散度在信息论中度量的是那个直观量

7.数值计算中的计算上溢与下溢问题，如softmax中的处理方式

8.与矩阵的特征值相关联的条件数(病态条件)指什么，与梯度爆炸与梯度弥散的关系

9.在基于梯度的优化问题中，如何判断一个梯度为0的零界点为局部极大值／全局极小值还是鞍点，Hessian矩阵的条件数与梯度下降法的关系

10.KTT方法与约束优化问题，活跃约束的定义

11.模型容量，表示容量，有效容量，最优容量概念

12.正则化中的权重衰减与加入先验知识在某些条件下的等价性

13.高斯分布的广泛应用的缘由

14.最大似然估计中最小化KL散度与最小化分布之间的交叉熵的关系

15.在线性回归问题，具有高斯先验权重的MAP贝叶斯推断与权重衰减的关系，与正则化的关系

16.稀疏表示，低维表示，独立表示

17.列举一些无法基于地图的优化来最小化的代价函数及其具有的特点

18.在深度神经网络中，引入了隐藏层，放弃了训练问题的凸性，其意义何在

19.函数在某个区间的饱和与平滑性对基于梯度的学习的影响

20.梯度爆炸的一些解决办法

21.MLP的万能近似性质

22.在前馈网络中，深度与宽度的关系及表示能力的差异

23.为什么交叉熵损失可以提高具有sigmoid和softmax输出的模型的性能，而使用均方误差损失则会存在很多问题。分段线性隐藏层代替sigmoid的利弊

24.表示学习的发展的初衷？并介绍其典型例子:自编码器

25.在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚

26.在深度学习神经网络中，所有的层中考虑使用相同的权重衰减的利弊

27.正则化过程中，权重衰减与Hessian矩阵中特征值的一些关系，以及与梯度弥散，梯度爆炸的关系

28.L1／L2正则化与高斯先验／对数先验的MAP贝叶斯推断的关系

29.什么是欠约束，为什么大多数的正则化可以使欠约束下的欠定问题在迭代过程中收敛

30.为什么考虑在模型训练时对输入(隐藏单元／权重)添加方差较小的噪声，与正则化的关系

31.共享参数的概念及在深度学习中的广泛影响

32.Dropout与Bagging集成方法的关系，以及Dropout带来的意义与其强大的原因

33.批量梯度下降法更新过程中，批量的大小与各种更新的稳定性关系

34.如何避免深度学习中的病态，鞍点，梯度爆炸，梯度弥散

35.SGD以及学习率的选择方法，带动量的SGD对于Hessian矩阵病态条件及随机梯度方差的影响

36.初始化权重过程中，权重大小在各种网络结构中的影响，以及一些初始化的方法；偏置的初始化

37.自适应学习率算法:AdaGrad，RMSProp，Adam等算法的做法

38.二阶近似方法:牛顿法，共轭梯度，BFGS等的做法

39.Hessian的标准化对于高阶优化算法的意义

40.卷积网络中的平移等变性的原因，常见的一些卷积形式

41.pooling的做法的意义

42.循环神经网络常见的一些依赖循环关系，常见的一些输入输出，以及对应的应用场景

43.seq2seq，gru，lstm等相关的原理

44.采样在深度学习中的意义

45.自编码器与线性因子模型，PCA，ICA等的关系

46.自编码器在深度学习中的意义，以及一些常见的变形与应用

47.受限玻尔兹曼机广泛应用的原因

48.稳定分布与马尔可夫链

49.Gibbs采样的原理

50.配分函数通常难以计算的解决方案

51.几种参数估计的联系与区别:MLE／MAP／贝叶斯

52.半监督的思想以及在深度学习中的应用

53.举例CNN中的channel在不同数据源中的含义

54.深度学习在NLP，语音，图像等领域的应用及常用的一些模型

55.word2vec与glove的比较

56.注意力机制在深度学习的某些场景中为何会被大量使用，其几种不同的情形

57.wide&deep模型中的wide和deep介绍

58.核回归与RBF网络的关系

此处问题很多编者本人也只有一个来自教材书籍的局部认识，望各位批评指正，可以在评论区留下正确全面的回答，共同学习与进步。

59.LSTM结构推导，为什么比RNN好？

60.过拟合在深度学习中的常见的一些解决方案或结构设计

61.怎么理解贝叶斯模型的有效参数数据会根据数据集的规模自动调整
