---
layout: article
title: 机器学习技法-决策树与集合算法
tags: 机器学习技法 adboosting GBDT
aside:
  toc: true
article_header:
  type: cover
  image:
    src: assets/images/background/pic/sky.jpg
mathjax: true
mathjax_autoNumber: true
---
# 决策树
什么是决策树？类似于数据结构中的树，将数据切开来，得到不同的结果。

使用决策树的好处？
- 可解释性，能够很好的理解结果是怎么产生的
- 多类别
- 类别而非数字输入
- 残缺资料
- 训练和使用都很有效率
## 如何构造决策树
我们可以设计构造二叉树，每次将数据一分为二，让其不纯度达到最低，当不纯度为0或者切下去数据分布都是一样的，那么就停止。此时二叉树可能会overfit，这时候再进行修剪，每次摘掉一片叶子，让$E_{in}$最小，设计摘多少片叶子即可。

# Blending
什么是blending?
- 多个算法集合起来得到更好的结果，就像是将多人智慧集合起来，一起来投票，得到一个更好的结果
<!--more-->
# Adaptive boosting
什么是Adaptive boosting

就像是给你一张试卷，要你打高分，你每做完一次，就调整每道题目的分值，让你总是打50分，比如说虽然你99道题目对了，1道题错了，但错的那道题占50分，这样你肯定要取把这道题做对。
最后$G(x)$就是把你每次做的结果综合起来，得到一个更好的结果。
流程如下：
>- 初始化权重$u_0^0=\frac{1}{n}$
>- $E_{in}^{u^{(t)}}=\frac{1}{N}\sum_{n=1}^{N}u_n^terr$
>>- if $y_n=wx+b$,then $u_{t+1}=u_t*ln(\diamondsuit t)$
>>- if $y_n\neq wx+b$,then $u_{t+1}=\frac{u_t}{ln(\diamondsuit t)}$
>- $\alpha_t=ln(\diamondsuit t)$
>- 循环若干次，得到$G(x)=sign(\sum_{t=1}^{T}\alpha_tg_t(x))$
>>其中
>>$\epsilon_t = \frac{\sum_{n=1}^{N}u_n^{(t)}[[y_n\neq g_t(x_n)]]}{\sum_{n=1}^Nu_n^{(t)}}$
>>$\diamondsuit t=\sqrt{\frac{1-\epsilon_t}{\epsilon_t}}$

## adboosting-Stump
每次并不使用所有的维度来进行预测，而是使用一个维度进行预测。最后将所有g(x)集合起来.

# 随机森林
什么是随机森林?

我们学过blending 和决策树，blending将资料D分成了一个个小部分资料拿去做预测，决策树可以将数据完整的切割。

一个是降低overfit，一个是会overfit，两者合一运用起来，并且做上一些修改。
- 将数据D分成若干份d
- 每个小数据集d并不是选择所有的特征，而是选择部分特征来构建决策树
- 将生成每棵树的数据作为其他树的验证数据，进行oob（out of bag）验证。
- 使用oob资料得到特征关联度，$importance(i)=E_{oob}-E_{oob}^{(p)}$,其中$E_{oob}^{(p)}$使用乱序的oob资料得到的err，这样得到每个特征的重要性。

随机森林理论上来说树越多越好。

# GDBT
## adaptive boosting的另一种解释
adaptive boosting+决策树

这里做两个修改
- 不直接权重，而是根据权重得到服从权重分布比例的数据，比如第一个数据的权重是0.1，第二个是0.9，那么构成的新数据是10%的第一个数据，90%的第二个数据。
- 修剪树的高度，使用限制树。这样做的目的是为了保证不会发生一棵树overfit，$\alpha_n=\infty$

当adaptive boosting为二分类时，$u_{t+1}与u_t$的关系式可以改为
- $\begin{aligned}
u_n^{t+1}&=u_n^t\diamondsuit t^{-y_ng_t(x_n)}\\
&=u_n^t\cdot exp(-y_n \alpha_n g_t(x_n))\\
&=u_n^1\cdot exp(-y_n\sum_{t=1}^T \alpha_n g_t(x_n))\\
&=\frac{1}{N}\cdot exp(-y_n\sum_{t=1}^T \alpha_n g_t(x_n))
\end{aligned}$

而且我们有 $G(x_n)=sign(\sum_{t=1}^T \alpha_n g_t(x_n))$

其中我们令voting score=$\sum_{t=1}^T \alpha_n g_t(x_n)$
这里voting score就像是SVM的未单位化的margin一样，它与$u_n^{t+1}$的关系是voting score越大，$u_n^{t+1}$则越小。
- voting score=$\sum_{t=1}^T \alpha_n g_t(x_n)$
- $u_n^{t+1}=\frac{1}{N}\cdot exp(-y_n\cdot voting score)$ 显然分数与$u_n^{t+1}$在voting score定义域是单调递减的

所以理论上来说，我们如果使得$u_n^{t+1}$越来越小，则G的效果会越来越好。
把$\sum_{n=1}^{N}u_n^{(t+1)}=\frac{1}{N}\sum_{n=1}^{N}exp(-y_n\sum_{t=1}^{T} \alpha_nx_n)$当成是err function

在梯度下降中我们有
- $E_{in}(w_t+\eta v)=E_{in}(w_t)+\eta v^T\nabla E_{in}(w_t)$

类似的可以得到
- $$\begin{aligned}
E_{ADA}&=\frac{1}{N}\sum_{n=1}^{N}exp(-y_n(\sum_{k=1}^{t-1}\alpha_kg_k(x_n)+\eta h(x_n)))\\
&=\sum_{n=1}^{N}u_n^{t}exp(-\eta y_n h(x_n)))\\
taylor展开&=\sum_{n=1}^{N}u_n^{t}(1-\eta y_n h(x_n)))\\
&=\sum_{n=1}^{N}u_n^{t}-\eta \sum_{n=1}^N y_nu_n^{t} h(x_n))
\end{aligned}
$$
> 这里$\eta 与h(x_n)$分别表示待定的步长与方向

当$-\sum_{n=1}^{N}y_nu_n^th(x_n)$越小，$E_{ADA}$也会越来越小
所以我们将问题更新为求 $min_{h}\ -\sum_{n=1}^{N}y_nu_n^th(x_n)$

当h(x)与$y_n$都是+1或-1时，则
- $$\begin{aligned}
-\sum_{n=1}^{N}y_nu_n^th(x_n)&=\sum_{n=1}^{N}u_n^{(t)}\left\{
\begin{aligned}
-1,if\ y_n&=&h(x_n)\\
+1,if\ y_n&\neq& h(x_n)
\end{aligned}
\right.\\
平移1&=-\sum_{n=1}^{N}u_n^{(t)}+\sum_{n=1}^{N}u_n^{(t)}(-y_nh(x_n)+1)\\
&=-\sum_{n=1}^{N}u_n^{(t)}+\sum_{n=1}^{N}u_n^{(t)}\left\{
\begin{aligned}
0,if\ y_n&=&h(x_n)\\
2,if\ y_n&\neq& h(x_n)
\end{aligned}
\right.\\
&=-\sum_{n=1}^{N}u_n^{(t)}+2E_{in}^{u^{(t)}}(h)
\end{aligned}
$$

而每一颗树都是在最小化$E_{in}^{u^{(t)}}(h)$得到一个好的g_t,而这个g_t正好是我们要求的方向h。

所以由原来的$E_{ADA}=\sum_{n=1}^{N}u_n^{t}exp(-\eta y_n h(x_n)))$

我们固定住h,用$g_t$代替，则变成

$\begin{aligned}
E_{ADA}&=\sum_{n=1}^{N}u_n^{t}exp(-\eta y_n g_t(x_n)))\\
&=(\sum_{n=1}^{N}u_n^{u^{(t)}})\cdot ((1-\epsilon_t)exp(-\eta)+\epsilon_t exp(+\eta))
\end{aligned}
$

对$\eta$求导令$\frac{\partial E_{ada}}{\partial \eta}=0$

得到$\eta=ln\sqrt{\frac{1-\epsilon_t}{\epsilon_t}}=\alpha_t$

所以总结下来就是
- $G(x)=sign(\sum_{t=1}^T \alpha_ng_t(x_n))$中的voting score=$\sum_{t=1}^T \alpha_ng_t(x_n)$，某种程度上来说跟svm的margin很相似，要么正的越多或者负的越多则表现会更好。比如做分类预测时，给出的结果总是0.99或者0.01 这样子的结果比0.6，0.7 要好的多。故当$y_n(voting\ socre)$比较大时，gdbt能取得比较好的结果。
- $y_n(voting\ socre)$  反比于$u_n^{T+1}$
- 要求$min\ u_n^{T+1}$等同于求
    - $min_{h}\ -\sum_{n=1}^{N}y_nu_n^th(x_n)$
- 等同于求
    - $-\sum_{n=1}^{N}u_n^{(t)}+2E_{in}^{u^{(t)}}(h)$
- 而每一颗树A都在求$E_{in}^{u^{(t)}}(h)$,且$g_t=h$
- 当固定$h=g_t$时，求最佳$\eta$,得到$\eta=\alpha_t$
## 将adaptive boosting的拓展成GBDT
- 使用任意的err
- 每一颗树使用决策树算法

比如GradientBoosting for Regression

设$err(s,y)=(s-y)^2$,我们求最佳的h,最佳的$\eta$

### 求最佳h
- $
\begin{aligned}
E_{gbdt}&=min_\eta \ min_h \frac{1}{N}\sum_{n=1}^{N}err(\sum_{k=1}^{t-1}\alpha_kg_k(x_n)+\eta h(x_n),y_n)\\
泰勒展开&=min_h\frac{1}{N}\sum_{n=1}{N}err(s_n,y)+\frac{1}{N}\eta h(x_n)\frac{\partial err(s,y_n)}{\partial s}\\
&=min_h 常数 + \frac{\eta}{N}\sum_{n=1}^{N}h(x_n)\cdot 2(s_n-y_n)\\
&=min_h 常数 + 常数\sum_{n=1}^{N}h(x_n)\cdot 2(s_n-y_n)\\
\end{aligned}
$

$h(x_n)$ 只是一个方向,大小无所谓,我们假设它越小越好.则有

$min_h 常数 + 常数\sum_{n=1}^{N}(2h(x_n)(s_n-y_n)+h(x_n)^2)\\
=min_h 常数 + 常数\sum_{n=1}^{N}(常数+(h(x_n)-(y_n-s_n))^2)$

求min$(h(x_n)-(y_n-s_n))^2$，做一个$h(x_n)$与余数$(y_n-s_n)$的回归,找最好的h

### 求最佳$\eta$
做一个简单线性回归求得最佳$\eta$

$min_\eta \frac{1}{N}\sum_{n=1}{N}(S_n+\eta g_t(x_n)-y_n)^2\\
=\frac{1}{N}\sum_{n=1}^{N}((y_n-s_n)-\eta g_t(x_n))^2$

$\alpha_t=optimal\ \eta$

### GBDT流程
初始$s_1,s_2,.....s_N=0$

for t=1,2,...T
1. 使用决策树做基础算法，每一颗树根据$x_n$与残差$(y_n-s_n)$得到最佳$g_t$
2. 做线性回归$(g_t(x_n),y_n-s_n)$得到$\alpha_t$
3. 更新$s_n=s_n+\alpha_tg_t(x_n)$

return $G(X)=\sum_{t=1}^T\alpha_tg_t(X)$
### GBDT总结
其实GBDT在做的事情就是看当前预测与实际还差多少，然后想办法，少什么添什么缺什么补什么，做得更好。
比如实际100分，预测90分，下一步考虑还差10分，做到最好。
