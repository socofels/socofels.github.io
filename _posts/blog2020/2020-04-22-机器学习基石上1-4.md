---
layout: article
title: 机器学习基石1-4
tags: 机器学习基石
aside:
  toc: true
article_header:
  type: cover
  image:
    src: assets/images/background/pic/sky.jpg
mathjax: true
mathjax_autoNumber: true
---
# 机器学习基石笔记
机器学习基石课程基本介绍：
- **这门课程是属于机器学习的基础理论，通过理论证明为什么机器可以学习到东西。为啥机器学习能成为现实。**
- 可以说是机器学习的可行性论证。

基本符号的解释：

符号|解释
:-|:-
g|最佳的猜想hypothesis
f|目标函数
F|资料
H|所有的可能性
x|表示人,
y|表示给不给信用卡
W|表示权重
threshold|门槛值
h|可能的公式





# binary问题基本的模型建立
$h(x)=sign(W_0x_0+W_1x_1+W_2x_2)$
这里的$W_0x_0$本来是一个门槛,把它简化后放到一起.说以我们常定义$x_0$为1,$W_0$就是一个常数，$h(x)$在图上表示出来就是一条线
<!--more-->
## 思路

- 接下来要做的,就是将这条线,转为向量问题。向量$W$($W_1,W_2$)
- 求得跟理想的线最近的那条线
- 根据每个错误点,调整W向量,得到新的W向量,每一次移动使得这条线比上次好一点.

$W^T_tx_n$|$y_nW^T_tx_n(t)$|
:-|:-|
表示得到的这个函数|如果这个式子大于0,那么久没有问题|

从数学角度上看又可以看做两个向量的内积,内积大于0表示向量夹角小,反之内积小于0,表示角度大。
- $$\vec{a}\cdot\vec{b}=|a||b|cos\theta$$

假设有一条完美的线,然后我们不断向他逼近.那么就有公式$W^T_fW_{t+1}>W^T_fW_{t}$

PLA 需要的条件,要线性可分,能用一条线把数据分为两个部分
### PLA到底会不会停下来
#### 首先我们假设有一个完美的函数f,它对应的线就是,$W_f$,我们现在PLA在找的函数是$W_t$,我们来看他们接不接近
1. 那么$min(y_nW^T_fx_n)>0$总是成立的
* 那么对于每个错误的点$x_n$也有
- $$y_{n(t)}W^T_fx_{n(t)}>min(y_nW^T_fx_n)>0$$
* 现在,我们看$W^T_fW_{t+1}$这两个向量是佛接近,转为数学问题就是求这两个向量的内积,如果他们内积越大,则说明他们越相似
- $$\vec{a}\cdot\vec{b}=|a||b|cos\theta$$
* 由之前学到的内容,我们可以得到公式
- $$W^T_fW_{t+1}=W^T_f(W_t+Y_{n(t)}x_{n(t)})$$
* $$W^T_fW_{t+1}=W^T_fW_t+Y_{n(t)}W^T_fx_{n(t)}$$
* 由上面不等式可以知道$min(y_nW^T_fx_n)>0$,所以,这两个数的内积$W^T_fW_{t+1}$看起来是越来越大的
* $W^T_fW_{t}>W^T_fW_0+T*min(y_nW^T_fx_n)$大于最小值
* $W^T_fW_{t}>T*min(y_nW^T_fx_n)$

#### 但是内积的公式为$\vec{a}\cdot\vec{b}=|a||b|cos\theta$,现在证明内积变大了,并不能完全说明是因为他们角度变小了,也有可能因为向量的长度变长了,接下来证明是不是因为长度的问题
1. 首先我们假设有一个出错的点$x_n$,那么就有不等式
- $$Y_{n(t)}W^T_tx_{n(t)}<=0$$
* $$W_{t+1}^2 = (W_t+y_{n(t)}x_{n(t)})^2$$
* 上式化简,并结合上面不等式,得到
- $$W_{t+1}^2 < |W_t|^2+max|y_{n}x_{n}|^2$$
- 意思是每次最多增长距离为最远那个点的距离.
* 由这个公式
- $$W_{t+1}^2 <= |W_t|^2+max|y_{n}x_{n}|)^2$$
- $$W_{t}^2 <= W_0^2+T*max|y_{n}x_{n}|^2$$
- $W_0$是个常量,这里我们可以设置为0向量,那么就有则有
- $$W_{t}^2 <= T*max|y_{n}x_{n}|^2$$
* 现在我们知道的消息是,这两个向量的长度在增加,向量内积也在增加.
* 好,那么接下来,我们求单位向量的内积,单位向量内积大小与长度无关,如果单位向量内积在变大,则足以说明这两个向量确实在靠近.而且两个单位向量的内积不是无穷大,最大值是1,所以我们可以得到
- $$\frac{W^T_f}{|W^T|}\frac{W_t}{|W_t|}<=1$$
* $$\frac{W^T_f}{|W^T_f|}\frac{W_t}{|W_t|}>\frac{T*min(y_nW_fx_n)}{|W^T_f|\sqrt{T|x_n|^2}}=\sqrt{T}*C$$
* **T乘以一个常数$<=$1,由此可得,T必定是有限的.**

## PLA需要假设有一个存在,PLA需要线性可分,PLA多久停下来

#### 第一个问题,如果说有杂讯,导致date不能线性可分怎么办
杂讯就是说原有的数据集有错误的点
* 找一条犯错误最少的线
* 把这条线放口袋里,再去找一条更好的线,如果有,那么久把更好的线放到口袋里,继续循环,循环到我们觉得还可以的时候停下来.

# 机器学习的问题分类
- 被动学习
    1. 监督学习supervisor learning
    * 非监督学习
    * 半监督学习
    * 神经网络学习
- 主动学习
    1. active 自动提出问题

## Learning with Different Protocol
* batch 批次学习
* online 学习,发现一次错误更新一次,例如的算法有PLA reinforcement
* active 主动学习

# 机器学习可能学不到的东西
* 机器学习对应一个训练集,可能产生不同的函数g
* 我们需要知道这个g好不好,就要与f进行对比,但是我们并不知道f
* 需要想一些工具,进行推论

#### 比如说我们有混合橘色,绿色弹珠的罐子,如何得到他们的比例是多少,如何进行推论
1. 抽样检查,我们设$u$罐子里面橘色弹珠的比率,$v$为抽样检查中橘色弹珠的比例
* 抽样检查有霍夫丁不等式hoeffding's inequality,$p[|v-u|>\epsilon]\leq2exp(-2\epsilon^2N)$
表示的是v和u相差大于$\epsilon$的几率小于等于$2exp(-2\epsilon^2N)$
* 只要样本足够多的,那么可以得到一个差不多的结论,probaly appromixmately correct(PAC)
* $$p[|v-u|>\epsilon]\leq2exp(-2\epsilon^2N)$$
    - N:是样本数
    - 我们不需要知道u是多少
    - 增大N或者增大间隙$\epsilon$就能得到我们想要的
* 通俗的表达就是，我随手一抓了一把弹珠，其中橘色弹珠占比v，这个v与实际占比u的误差在$\epsilon$的概率是多少。
    - 比如v=50%，$\epsilon=5\%$,那么公式$p[|v-u|>\epsilon]$的意思就是，我说这瓶弹珠有50%是橘色弹珠，误差在5%以上的概率小于$2exp(-2\epsilon^2N)$。
* 这个公式计算,bound就是$2exp(-2\epsilon^2N)$

#### 继续用弹珠做例子,假设罐子里每一个弹珠为一个x,绿色弹珠表示$h(x)=f(x)$,橙色弹珠表示$h(x)\neq f(x)$
* 抽一百个样,可以得到有多少个跟f(x)结果不一样

## week4 connection to learning
Eout：
- 在整个罐子中,$h(x)\neq f(x)$的几率有多大
- $$E_{out}=\epsilon_{x-p}[h(x)a\neq f(x)]$$

Ein：
- $$E_[in]=\frac{1}{N}\sum_{i=1}^{N}[h(x_n)\neq y_n]$$
- 我们手上有的样本中$h(x)\neq f(x)$的比率
* 将$E_{out}$和$E_{in}$带入霍夫丁式子中,可以得到
- $$P|E_{in}-E_{out}|>\epsilon\leq 2exp(-2\epsilon^2N)|$$
- 得到边界的意思是,到底有多大把握,这个你手里的$E_{in}$跟我的$E_{out}$是一样的.
* 我们需要找的就是找一个h,使得$E_{in}$很小[错误率很小],而且$E_{in}$与$E_{out}$非常接近
* hoeffding 得到的就是错误率为多少

### 分析抽样的概率
#### 如果你有很多h,选择其中一个进行抽样检查,得到的$E_in$很小,那么这个$E_in$,我们要不要选它当做g?
用硬币做比喻,150个人,连续投硬币五次,有99的概率至少有一个人连续5次硬币都朝上.
也就是说,我们抽样检查有可能得到特例的$E_in$,它与实际的$E_out$相差非常遥远
* 不好资料,就是说演算法不能自由选择,有可能踩雷
* 不好的资料bad data,抽一把资料,这个资料是特例$E_in$与$E_out$相差很大
* 我们对所有资料D和算法h创建列表,对于每个D,如果有一个h说是这个数据不行,我们就把这个数据当做bad data.
#### 现在我们要想知道,在所有的data和M个hypotheses中,有多少D是不好的?这里的每个hypotheses对应每个${E_in}$最好的那个hypotheses
求
1. $$P_D[BAD D]=[badD for h_1 or badD for h_2 or badD for h_3 ........badD for h_m]$$
    - 把or转为+,那么得到不等式
* $$P_D[BAD D]\leq P_D[badD for h_1]+P_D[badD for h_2]+.......P_D[badD for h_m]$$
    - 可以这么说,对于每一块数据,我给不同的人去做分析,那么不论是谁来做,都有可能出现结论与实际不符,
    这个出现的概率是一定的,为$$2exp(-2\epsilon^2N)$$
    - 对于每个$$P_D[badD for h_n]$$由hoeffding可以得到
        - $$2exp(-2\epsilon^2N)|$$
* $$P_D[BAD D]\leq 2Mexp(-2\epsilon^2N)$$
    - 这个公式的意思就是说，虽然你数据中有一些垃圾，但只要你的数据量（N）足够庞大，那么机器也能够学到东西。
* 所以从理论上来说,我们就能找到一个$E_{in}(g)=E_{out}(g)$,不论怎么选,都能选到一个g
* 在这个情况下,我们选$E_{in}$最小的g
* 所以我们得到结论,机器学习是有可能做得到在线性不可分的情况下,找到一条最好的线.


```python
# 假设epsilon，求它的边界
from numpy import *
def hoeffding(M,epsilon,N):
    return 2*M*exp(-(2*epsilon*epsilon*N))
param=[[100,0.1,100],
 [100,0.1,200],
 [100,0.1,400],
 [100,0.1,800],
 [100,0.1,1600],
 [1000,0.1,3200],
 [1000,0.1,6400],
 [1000,0.1,12800],
 [1000,0.1,25600],
]
for M,epsilon,N in param:
    print(f"当M:{M}，epsilon:{epsilon}，N:{N}时\n错误边界为:{hoeffding(M,epsilon,N)}\n")

```

    当M:100，epsilon:0.1，N:100时
    错误边界为:27.067056647322524

    当M:100，epsilon:0.1，N:200时
    错误边界为:3.663127777746833

    当M:100，epsilon:0.1，N:400时
    错误边界为:0.06709252558050226

    当M:100，epsilon:0.1，N:800时
    错误边界为:2.2507034943851743e-05

    当M:100，epsilon:0.1，N:1600时
    错误边界为:2.5328331098188172e-12

    当M:1000，epsilon:0.1，N:3200时
    错误边界为:3.2076217810972303e-25

    当M:1000，epsilon:0.1，N:6400时
    错误边界为:5.144418745284684e-53

    当M:1000，epsilon:0.1，N:12800时
    错误边界为:1.323252211341822e-108

    当M:1000，epsilon:0.1，N:25600时
    错误边界为:8.754982074105107e-220



由此可以看出来，只要你的数据量够大，结论与实际不符的概率会变得极其渺小，所以。机器学习概率是可行的。

## 拓展 伯努利模型
* 独立重复的实验进行n次,在每次实验中$P(A)=p(0<p<1)$,则在重伯努利实验中A发生k次的概率为二项概率公式
* $C^k_np^k(1-p)^{n-k},k = 0,1,2,.....n$









