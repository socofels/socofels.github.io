---
layout: article
title: 机器学习基石下9-11
tags: 机器学习基石
aside:
  toc: true
article_header:
  type: cover
  image:
    src: assets/images/background/pic/sky.jpg
mathjax: true
mathjax_autoNumber: true
---
# 机器学习在线性回归问题的应用
### 总体思路:
- 得到目标函数h(x)
- 得到损失函数$E_{in}$,
- 求解$E_{in}$取得极小值时的W。

### 具体实现：
- 构建函数,我们需要找到一个权重W,使得误差分析函数$E_{in}=0$
* 函数的构建很简单,以及该函数的误差分析函数$E_{in}$
    - $$h(x)=X^TW$$
    - $$E_{in}=(h(x)-y)^2$$
* 有了这两个式子,我们知道,当$E_{in}=0$时,我们就得到了想要的解,W.我们知道,误差分析函数$E_{in}$就是每个点,离这条线的距离,那么要使得$E_{in}=0$,那就要求所有点都在一条线上,而实际上,这些点都是分散的,一般不会再一条线上,那么我们需要办法,找到误差分析函数$E_{in}$中最小的那个.
* 把上面一式带入二式子得到
    - $E_{in}=(X^TW-y)^2$
* 有了这个式子后,我们首先要明白他的物理意义,实际上在2维图上就是一个抛物线,凸函数,那么我们怎样找到它的最低点呢?我们知道凸函数的导数为0时,就是最小值,在多维时,每一维度的$E_{in}(W_n)$都是0,那么说干就干
    - 我们来求$E_{in}$的导数,用$\nabla E_{in}$来表示
    - $\nabla E_{in}=\frac{2}{N}(X^TXW -X^Ty)$
    - $\nabla$ 符号代表算子,向量微分算子,在一维表示倒数,在多维表示梯度
* 找到梯度$\nabla E_{in}=0$时,的W.
    - $0=\frac{2}{N}(X^TXW -X^Ty)$
    可以得到
    - $W_{lin}=(X^TX)^{-1}X^Ty$
    注意:这里式子中的$(X^TX)^{-1}X^T$我们把它称之为pseudo-inverse(伪逆矩阵),用符号$X^+$表示.*这里特别之处在于,当$(X^TX)$为不可逆矩阵时,这个$X^+$会是一个同等效用的矩阵,从而在设计程序时,不用去考虑$(X^TX)$可不可逆的这种问题.*
* 最终就可以得到公式
    - $$W_{lin} = X^+y$$

#### 那么线性回归问题就解决了吗?我们讨论的这些没有考虑到杂讯,当有杂讯的时候,又会是什么样子呢?
- 有学者对错误率的平均值进行了研究,发现规律
    - $\bar{E_{in}}=noise level\cdot(1-\frac{d+1}{N})$
    - $\bar{E_{out}}=noise level\cdot (1+\frac{d+1}{N})$
    - 这两个函式子意味着,当N越来越大时,$\bar{E_{in}}$会会越来越小$\bar{E_{out}}$会越来越大,他们就会越来越靠近.

#### 拓展
* 线性回归问题会得到实数的答案,而二分类问题会得到+1与-1的答案,我们画出他他们的错误边界,可以知道,线性回归问题的错误分析函数比而分类问题的错误分析函数要大,这意味着我们可以用线性回归的错误分析函数来代替二分类的错误分析函数.

# logistic regression 逻辑线性回归
## 我们要知道预测病人得病的概率为多少,这个如何求?
### 总体思路与线性回归一致:
- 得到目标函数h(x)
- 得到损失函数$E_{in}$,
- 求解$E_{in}$取得极小值时的W。

### 具体实现
- sigmoid函数的分布：
    - ![png](https://socofels.github.io/assets/images/blogimg/sigmoid.png)
- 我们要知道概率,那么就是要将线性回归得到的分数,转为概率,这里我们可以采用sigmoid函数,将任意实数返回区间(-1,1)的实数.sigmoid函数如下:
    - $\theta(s)=\frac{1}{1+e^{(-s)}}$
* 将得分$W^TX带入sigmoid函数就可以得到:$
    - $$h(x)=\frac{1}{1+e^{(-W^TX)}}$$
* 接下来我们求它的错误分析函数$E_{in}$
    - 首先,假设f存在,那么$f(x_n)$就是样本$x_n$在y上为正时的概率
    - 我们假设有一个克隆人h,它跟f一模一样,那我们反过来推,它产生数据集的概率跟f也是一模一样,就可以得到
        - $P(x_1)f(x_1)*....*P(x_n)f(x_n)=P(x_1)h(x_1)*...*P(x_n)h(x_n)$
    - 这里有一点要提一下,因为sigmoid函数
        - $\because\theta(-s) = - \theta (s)$
        - $\therefore1 - h(x) = h(-x)$
    - 得到h的概率为
    - $$P(x_1)h(x_1)*...*P(x_n)h(1-h(x))$$
    - 我们用$likelihood(h)$表示复制人产生数据D的概率是多少
    - $$likelihood(h) =P(x_1)h(x_1)*...*P(x_n)h(h(-x_n))$$
    - 把常数$P(X_1)*..*P(X_n)$去掉
        - $$max likelihood(h)\propto \prod^N_{n=1}h(y_nx_n) $$
    - 将上式进行下列操作变形
        - 最大改为最小:乘以-1,
        - 对h(x)取对数ln,$\prod^N_{n=1}h(y_nx_n)$变为$\sum^N_{n=1}ln\theta(y_nW^Tx_n)$
        - 增加一个常数$\frac{1}{N}$
        - $$likelihood(h)\propto min\frac{1}{N}\sum^N_{n=1}-ln\theta(y_nW^Tx_n)$$
    - 将sigmoid函数带入进来，最终得到costfunction：
    - $$err(W,x,y)=ln(1+exp(-yWX))$$
    - $$minE_in(w) =\frac{1}/{N}\sum^{N}_{n=1}ln(1+exp(-y_nW^TX_n))$$
* 由已知结论可得,min$E_in(w) =\frac{1}/{N}\sum^{N}_{n=1}ln(1+exp(-y_nW^TX_n))$是一个凸曲线,那么我们还是走之前的老路,对他求导,导数为0时就有最小值.求导后有函数
    - $\nabla E_{in}=\frac{1}{N}\sum^N_{n=1}\theta(-y_nW^TX_n)(-y_nx_n)$
    - 对这个式子进行分析,我们能否找到$\nabla E_{in}=0$的点呢?
    - $\nabla E_{in}=\frac{1}{N}\sum^N_{n=1}\theta(-y_nW^TX_n)(-y_nx_n) = 0$
        * 意味着$\theta(-y_nW^TX_n)\to 0$
        * $-y_nW^TX_n \to -\infty$
        * $y_nW^TX_n \to \infty$
        * 也是是说我们输入的每一个s都得是正的，我们理想中的函数必须能够完美预测所有数据，不能有一个错误。
* 这意味着所有点必须线性可分,这通常是做不到的,我们可以想到之前的PLA与pockert方法,知错能改,错一个改一个,试试看能否运用到这里来.
    - $W_{t+1}\gets w_t + \eta v$ 这里 $\eta$就是步长,v就是方向求梯度.
    - v = $-\nabla E_{in}$  这个物理意义就是,往梯度的反方向走.

### 使用新的err
* 目标函数保持不变。
* $$h(x) =sign(W^Tx)$$
* 原来当h = y时$err= 0 $;当$h \neq y$时$err= 1 $
* 现在我们用新的$\hat{err}$来作为err的上届,我们使用特定的函数,称为sce
    - $$\hat{err}=ln(1+exp(-ys))$$

### 如何求得极值点？
#### 四个点出发,h(x),E(in),梯度,求得W
- 我们将s带入进去：
    - $$S= W^TX$$
    - $$E(in) = \frac{1}{N}\sum^N_{n=1}ln(1+e^{-y_nW^TX_n})$$
* 计算梯度
    - $$\nabla E_{in}=\frac{1}{N}\sum^N_{n=1}\frac{e^{-y_nW^TX_n}}{(1+e^{-y_nW^TX_n})}(-y_nX_n)$$
    - 化简得：
    - $$\nabla E_{in}=\frac{1}{N}\sum^N_{n=1}\theta(-y_nW^TX_n)(-y_nX_n)$$
* $W_{t+1} = W_t - \eta V$  这里$\eta$就是步长,$V$就是方向的向量
    - $$W_{t+1} = W_t - \eta \nabla E_{in}$$
    - $$W_{t+1} = W_t - \eta(\frac{1}{N}\sum^N_{n=1}\theta(-y_nW^TX_n)(-y_nX_n))$$
    - 当$\nabla E_{in}$接近0时，可以得到不错的结果

# 线性回归与逻辑回归的联系

- ![png](https://socofels.github.io/assets/images/blogimg/ml2.jpg)

- 由上图可了解，线性回归的err是逻辑回归的上界，意味着我们可以使用线性回归err代替逻辑回归的err，当线性回归err降低的时候，逻辑回归err也会变低。
- 上图对交叉伤损进行了缩放，改为使用2为底的对数，其实区别不大，只是在y=1时，刚好和err(0/1)的值一样，这样以来线性回归err就严格大于逻辑回归了。

### 对于梯度下降的优化,随机梯度下降
- 随机梯度的核心就在于获得的梯度grad,$\nabla E_{in}$,我们不是对所有的点都取值,而是随机找一个点,使用这个点的梯度来代替整体梯度。
    - 随机梯度下降之所以能够有效代替梯度下降是因为理论上来说，只要我们取的点够多，那么随机梯度的平均值是很接近整体的梯度的，但是它也有缺点，缺点是它最终的结果会在目标值附近波动，而不会精确得到目标值。

# 多类别分类问题
之前学过0/1的分类问题，所有问题只有正反两种结果，现在如果说一个问题，有四种类别的结果，那么0/1就不行了，我们就要使用别的方法来解决这样的问题。我们有多种方法可以解决。
- one vs all
- one vs one
- softmax（未讲）

### 什么是one vs all？
比如说有四个类别 方块，红桃，黑桃，梅花。那么我们把它当做4个二分类问题，方块-其他，红桃-其他，黑桃-其他，梅花-其他。这样就实现了多类别分类。
- one vs all 的缺点：
    - 当类别很多时，比如有100个类别，那么分类器全部猜其他，也有99%的正确率，此时效果可能会非常差。

### 什么是one vs one
比如说有四个类别 方块，红桃，黑桃，梅花。每次抽出两个类别的数据进行训练，最后综合每次的训练结果，进行一个投票。

- 优点：
    - 效率高。因为每次训练只要拿出其中两个部分的数据。
- 缺点：
    - n个类别就需要进行n!次训练，每一次训练就要获得一个W，那么需要储存非常的w，很占内存。

### sotfmax
softmax分类器得出每一个类别的概率是多少，选最高的那个作为结果。是在logstic上进行拓展得来。
- 目标函数:
    - $$h(x^i)=\frac{1}{\sum_{j=1}^ke^{w_(j)x^(i)}}
\left[
 \begin{matrix}
   e^{w_(1)x^(i)}\\
   e^{w_(2)x^(i)} \\
   ...\\
   e^{w_(k)x^(i)}\\
  \end{matrix}
\right]
$$

- err函数：
    - $$err=\sum_{k=1}^n\sum_{i=1}^my_klog^{h(x^(i))_k}+(1-y_k)log^{1-h(x^(i))_k}$$
    - m表示总样本数量，n表示总类别个数。

