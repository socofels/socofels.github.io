---
layout: article
title: 机器学习技法-SVM
tags: 机器学习技法 SVM
aside:
  toc: true
article_header:
  type: cover
  image:
    src: assets/images/background/pic/sky.jpg
mathjax: true
mathjax_autoNumber: true
---
# SVM介绍
## 什么是SVM
SVM（support vector machine）支撑向量机又名大间距分类器，指寻找一条决策线，使得正负样本之间的间距最大。
## 为什么要用SVM
SVM重要的是使用了核函数，将变量维度从低维拓展到了无限维，虽然其他方法也可以使用核函数，但SVM得到的结果也比其他方法简单许多，预测结果时比其他方法快。其唯一不足之处在于它样本数据很大时比求解成本较高，但在中小型数据应用十分广泛，这也是SVM方法在机器学习大行其道的原因。

# SVM的实现
## hard SVM
hard svm 是相对于soft svm来说的，这里hard指所有的数据都能被完美的线性可分。
### 定义目标函数
我们要做的是找到一条线使得数据一分为二，且每个点与该线的最近的点的距离要是所有线中最大的。则有
>$$max\ margin$$
>$$S.T\ \  y_n(XW+b)>0$$
>$$min_{n=1~M}\ margin:distance\{X_n\}$$
<!--more-->
理解这个可以打个比喻，从每个班级中选出一个最瘦的同学进行评比(条件)，再从所有参加评比的同学中选一个最胖作为冠军(目标)。

如何求解该问题？这并不是一个可以求导的连续函数，那么这个问题求解的方式我们的思路是把它转为二次规划问题来求解。
- 把$distance\{X_n\}$看做作是$X_n$与垂直于$W$的平面上任意一点的连线，在$W$向量上投影的模，则可得。
>$$max\ margin$$
>$$S.T\ \  y_n(XW+b)>0$$
>$$min_{n=1~M}\ margin:\lvert\frac{W}{\lvert W \rvert}X_n+b\rvert$$
- $\because$有条件$y_n(XW+b)>0$，$$\therefore\lvert\frac{W}{\lvert W\rvert }X_n+b\rvert>0$$，可以乘上$y_n$把绝对值符号去掉。
- 另外我们把$$min_{n=1~M}\ margin:\lvert\frac{W}{\lvert W\rvert}X_n+b\rvert$$看作是以最小的那个作为基准，即$(WX_n+b)y_n=1$,而长度就为$$\frac{1}{\lvert w\rvert}$$,可以这样比喻，以最轻的那个同学体重为基准1，不管他的重量是一个八戒还是一个嫦娥那么重都没关系，反正以他的体重为基准,那么班上所有同学的体重都是他体重的一倍以上。
- 同时既然有$$\frac{1}{\lvert w\rvert}(WX_n+b)y_n\geq1$$则已包含了条件$y_n(XW+b)>0$
>$$max\ \frac{1}{\lvert w\rvert}$$
>$$S.T\ \ \ (WX_n+b)y_n\geq1$$
- 为了应用于二次规划，将求最大值改为求最小值.得到**二次规划标准型**，可以带入二次规划程序求解。
>$$min\ \frac{1}{2}W^TW$$
>$$S.T\ \ \ (WX_n+b)y_n\geq1$$

### 将标准型转为对偶问题
为什么要将标准型的二次规划问题转为他的对偶问题呢？
- 标准型中的$W$在求解时若维度很大时，求解效率变得非常低。标准型的目标函数有d+1个变量，有N个条件。转为对偶问题让变量与d无关。
- 引入一件大杀器，核函数。

如何转为对偶问题呢？
- 对偶问题就是将条件整合到目标函数中去我们有了标准型之后可以这样操作，我们有条件$0\geq1-(W^TX_n+b)y_n$
>$$min( \frac{1}{2}W^TW+\sum_{n=1}^{N}\alpha_nmax(1-(W^TX_n+b)y_n,0)$$
>- 使用函数来表示
>$$L_{(b,w,\alpha)}= \frac{1}{2}W^TW+\sum_{n=1}^{N}\alpha_n(1-(W^TX_n+b)y_n,0),all\ \alpha_n\geq0$$
> $$求min(max(L_{(b,w,\alpha)})),all\ \alpha_n\geq0$$
>- $max(1-(W^TX_n+b)y_n,0)$这里头的意思是说，两者之间取大的值，当$1-(W^TX_n+b)y_n>0$时，那么这个W并不能将所有数据正确分开，反之小于等于0时是线性可分的,另其固定等于0。待会儿取最小值的时候会过滤掉那些大于零的项。


- 设想如果$\alpha'$是所有$\alpha_n$中的一个，那么可以有不等式

>$$min(max(L_{(b,w,\alpha)}))\geq min(L_{(b,w,\alpha')}),all\ \alpha_n\geq0 ,\alpha'\in \alpha_n$$

- 既然对每一个$\alpha'$都满足上面那个公式，那么有

>$$min(max(L_{(b,w,\alpha)}))\geq max(min(L_{(b,w,\alpha')})),all\ \alpha_n\geq0,all\ \alpha'_n\geq0$$

得到了这个若对偶关系，如果满足下列条件的则为强对偶关系。
- 是凸问题
- 原问题有解
- 有线性条件

这里都满足，所以得到强对偶关系。之所以将最小化和最大化调换顺序取解右边的式子是为了找最佳解(w和b)的时候不受条件限制，容易获得解。
所以我们在这里只要求解右边问题，其解等于左边问题的解。
>$$min(max(L_{(b,w,\alpha)}))= max(min(L_{(b,w,\alpha')})),all\ \alpha_n\geq0,all\ \alpha'_n\geq0$$

求解$max(min(L_{(b,w,\alpha)})),all\ \alpha_n\geq0$，依次对b,w求偏导即可得到关系式
>$$\frac{\partial L_{(b,w,a)}}{\partial b}=0$$
>$$\frac{\partial L_{(b,w,a)}}{\partial w}=0$$

求导后带入上式可得

>$$max(-\frac{1}{2}|\alpha_ny_nz_n|^2+\sum_{n=1}^N\alpha_n),条件(\ all\ \alpha_n\geq0,\sum_{n=1}^Ny_n\alpha_n=0,\sum_{n=1}^Ny_nz_n=w)$$

当对偶问题与原问题满足KKT条件则对偶问题的解=原问题的解：
- 原问题可行性
- 对偶问可行性
- 互补松弛条件
- 稳定性条件

该对偶问题均满足KKT条件，故求出对偶问题的解等同于求出原问题的解

将求最大值改为求最小值，喂给二次规划程序即可求得解。
>$$min(\frac{1}{2}|\alpha_ny_nz_n|^2-\sum_{n=1}^N\alpha_n),条件(\ all\ \alpha_n\geq0,\sum_{n=1}^Ny_n\alpha_n=0,\sum_{n=1}^Ny_nz_n=w)$$

## softSVM
softsvm 是相对于hardsvm来说的，当数据不能够完全线性可分时，也能求得解
### 如何实现
在hardsvm中，初始条件是$y_n(XW+b)>0$,即数据完全线性可分，一个都不能犯错，我们需要把这个条件改为可以犯错，但是犯的错要最少。

假设我们将条件设为，犯错就记录错了多少,记为$\zeta$，则有：
>$$min\ \frac{1}{2}W^TW+C\sum_{n=1}^N\zeta_n$$
>$$S.T\ \ \ (WX_n+b)y_n\geq1-\zeta_n$$
>$$\zeta_n\geq0\ for\ all\ n$$
对应的对偶问题可以得到:

>$$max(min\ \frac{1}{2}W^TW+C\cdot \sum_{n=1}^N\zeta_n+\alpha_n(1-\zeta_n-(WX_n+b)y_n)+\sum_{n=1}^N\beta_n\cdot(-\zeta_n))$$
>$$\alpha_n>0,\beta_n>0$$

由$$\frac{\partial L(w,b,\alpha,\beta,\zeta)}{\partial \zeta_n}=0=C-\alpha_n-\beta_n$$
可得
>$$max(min\ \frac{1}{2}W^TW+\alpha_n(1-(WX_n+b)y_n)+\sum_{n=1}^N\beta_n\cdot((C-\alpha_n-\beta_n)\zeta_n))$$
>$$\alpha_n\geq0,\beta_n\geq0,C-\alpha_n-\beta_n=0$$

## kernel的运用
### 为什么要使用核函数
当x从低纬组合到高纬时，维度会非常迅速的增加，当维度特别大时，w特别大，z也特别大，难以求解，引入核函数则可简单求解$Z^TZ$。

将两步合二为一,从而更快求解。
- X到Z的映射
- 內积$Z^TZ$

设$K(\phi(x),\phi(x'))=\phi(x)^T\phi(x')$

$\phi(x)=(1,x_1,x_2.......x_1x_2,...x^2_d)$

任意Q次幂的kernel有
- $$K^Q=(\xi+\lambda XX')^Q$$
常用的有$$exp(-r(x-x')^2)$$
$$r>0$$
### 除了在svm中使用kernel，在其他方法也使用kernel吗，比如岭回归等。
可以使用， 但不常使用，因为没有优势，当SVM使用kernel时，求得的$\alpha_n$有很多是0，预测时就很迅速，而其他方法使用kernel大多数都不为0，没有明显优势。

# 什么是SVR？
SVM是分类器，通过之前的学习，我们知道分类与回归其实是可以通用的。

我们将softmax条件改为若点在回归线附近则不计算错误，若离回归线很远则计算错误。

>$$min\ \frac{1}{2}W^TW+C\sum_{n=1}^Nmax(0,|W^TZ_n+b-y_n|-\epsilon)$$
>- $epsilon$为回归线的宽度

>这里有绝对值，不能直接求导，设$\zeta_n^{\bigvee},\zeta_n^{\bigwedge}$分别为最远误差距离的下界和上界，于是转化问题为
>$$min\ \frac{1}{2}W^TW+C\sum_{n=1}^N(\zeta_n^{\bigvee}+\zeta_n^{\bigwedge})$$
>$$S.T\ \ -\epsilon-\zeta_n^{\bigvee}\leq y_n-W^TZ_n-b\leq \epsilon+\zeta_n^{\bigwedge}$$
>$$\zeta_n^{\bigvee}\geq0,\zeta_n^{\bigwedge}\geq0$$

转为对偶问题，使用拉格朗日乘子$\alpha_n^{\bigvee},\alpha_n^{\bigwedge}$
>$$min\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}(\alpha_n^{\bigwedge}-\alpha_n^{\bigvee})(\alpha_m^{\bigwedge}-\alpha_m^{\bigvee})K_{n,m}+\sum_{n=1}^{N}((\epsilon-y_n)\cdot \alpha_n^{\bigwedge})+(\epsilon+y_n)\cdot \alpha_n^{\bigvee})$$
>$$S.T\ \sum_{n=1}^N(\alpha_n^{\bigwedge}-\alpha_n^{\bigvee})=0$$
>$$0\leq \alpha_n^{\bigwedge}\leq C_i$$
>$$0\leq \alpha_n^{\bigvee}\leq C_i$$

# 总结
支撑向量机其实跟岭回归很是相似，这里我们求的是$min\frac{1}{2}W^TW$再加上一个条件的拉尔朗日乘子，而岭回归loss函数是求最小的损失加上$\frac{1}{2}W^TW$

在感知机中我们有
- $$W=W+y_nz_n$$

在svm中我们有
- $$W=\alpha_ny_nz_n$$
